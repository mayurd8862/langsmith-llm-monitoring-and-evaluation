{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quickstart docs: https://docs.smith.langchain.com/evaluation/tutorials/evaluation\n",
    "- Langsmith Evaluation Dashboards: https://smith.langchain.com/o/f602a986-7b0f-5e2b-aa00-74708f7af432/datasets/cab1b44f-92cb-4aa0-8b0e-55891dd4271f/compare?selectedSessions=8d9f3ca8-1784-4a3d-95af-75252aac219f%2C45f67b78-78b8-40dd-8134-aee86e870bfc&baseline=8d9f3ca8-1784-4a3d-95af-75252aac219f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "from langsmith.evaluation import evaluate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "from langsmith import wrappers, Client\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "LANGCHAIN_API_KEY = os.environ.get(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"evaluation_demo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = Client()\n",
    "\n",
    "# Create inputs and reference outputs\n",
    "examples = [\n",
    "    # Geography\n",
    "    (\n",
    "        \"Which country is Mount Kilimanjaro located in?\",\n",
    "        \"Mount Kilimanjaro is located in Tanzania, a country in East Africa known for its diverse wildlife and stunning landscapes.\"\n",
    "    ),\n",
    "    (\n",
    "        \"What is Earth's lowest point?\",\n",
    "        \"Earth's lowest point is the Dead Sea, located at the border between Jordan and Israel, with a surface elevation of about 430 meters below sea level.\"\n",
    "    ),\n",
    "    # Science\n",
    "    (\n",
    "        \"What is the chemical symbol for water?\",\n",
    "        \"The chemical symbol for water is H₂O, which represents two hydrogen atoms bonded to one oxygen atom.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Who developed the theory of relativity?\",\n",
    "        \"The theory of relativity, which revolutionized our understanding of space, time, and gravity, was developed by Albert Einstein in the early 20th century.\"\n",
    "    ),\n",
    "    # History\n",
    "    (\n",
    "        \"Who was the first President of the United States?\",\n",
    "        \"The first President of the United States was George Washington, who served from 1789 to 1797 and played a key role in leading the country during its formative years.\"\n",
    "    ),\n",
    "    (\n",
    "        \"When did World War II end?\",\n",
    "        \"World War II ended in 1945, marking the conclusion of one of the most significant and devastating conflicts in human history.\"\n",
    "    ),\n",
    "    # Literature\n",
    "    (\n",
    "        \"Who wrote 'Pride and Prejudice'?\",\n",
    "        \"'Pride and Prejudice' was written by Jane Austen, a renowned English novelist known for her insightful commentary on society and human relationships.\"\n",
    "    ),\n",
    "    (\n",
    "        \"What is the title of Shakespeare's play about the Prince of Denmark?\",\n",
    "        \"The title of Shakespeare's play about the Prince of Denmark is 'Hamlet', a tragedy that explores themes of revenge, madness, and morality.\"\n",
    "    ),\n",
    "    # Technology\n",
    "    (\n",
    "        \"What does CPU stand for in computing?\",\n",
    "        \"CPU stands for Central Processing Unit, which is the primary component of a computer responsible for executing instructions and performing calculations.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Who is considered the founder of Microsoft?\",\n",
    "        \"Microsoft was founded by Bill Gates and Paul Allen in 1975, and it has since become one of the leading technology companies in the world.\"\n",
    "    ),\n",
    "    # General Knowledge\n",
    "    (\n",
    "        \"What is the capital of Japan?\",\n",
    "        \"The capital of Japan is Tokyo, a bustling metropolis that is the political, economic, and cultural center of the country.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Which planet is known as the Red Planet?\",\n",
    "        \"Mars is known as the Red Planet due to its reddish appearance caused by iron oxide, or rust, on its surface.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "inputs = [{\"question\": input_prompt} for input_prompt, _ in examples]\n",
    "outputs = [{\"answer\": output_answer} for _, output_answer in examples]\n",
    "\n",
    "# Programmatically create a dataset in LangSmith\n",
    "dataset = client.create_dataset(\n",
    "  dataset_name=\"Sample dataset\", description=\"A sample dataset in LangSmith.\"\n",
    ")\n",
    "\n",
    "# Add examples to the dataset\n",
    "client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the application logic you want to evaluate inside a target function\n",
    "# The SDK will automatically send the inputs from the dataset to your target function\n",
    "def target(inputs):\n",
    "  \n",
    "    llm = ChatGroq(model=\"mixtral-8x7b-32768\")\n",
    "    res = llm.invoke(inputs)\n",
    "\n",
    "    return res.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output schema for the LLM judge\n",
    "class Grade(BaseModel):\n",
    "    score: bool = Field(\n",
    "        description=\"Boolean that indicates whether the response is accurate relative to the reference answer\"\n",
    "    )\n",
    "\n",
    "def accuracy(outputs: dict, reference_outputs: dict) -> bool:\n",
    "    # Set up a parser + inject instructions into the prompt template.\n",
    "    parser = JsonOutputParser(pydantic_object=Grade)\n",
    "    model = ChatGroq(model=\"mixtral-8x7b-32768\")\n",
    "    prompt = PromptTemplate(\n",
    "        template=f\"\"\"Evaluate Student Answer against Ground Truth for conceptual similarity and classify true or false: \n",
    "        - False: No conceptual match and similarity\n",
    "        - True: Most or full conceptual match and similarity\n",
    "        - Key criteria: Concept should match, not exact wording.\n",
    "\n",
    "        Ground Truth answer: {reference_outputs}\n",
    "        Student's Answer: {outputs}\n",
    "        \"\"\",\n",
    "        input_variables=[\"query\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    chain = prompt | model | parser\n",
    "\n",
    "    return chain.invoke()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(experiment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running the evaluation, a link will be provided to view the results in langsmith\n",
    "experiment_results = evaluate(\n",
    "  target,\n",
    "  data=\"Sample dataset\",\n",
    "  evaluators=[\n",
    "      accuracy,\n",
    "      # can add multiple evaluators here\n",
    "  ],\n",
    "  experiment_prefix=\"first-eval-in-langsmith\",\n",
    "  max_concurrency=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "from langsmith.evaluation import evaluate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "from langsmith import wrappers, Client\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "LANGCHAIN_API_KEY = os.environ.get(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"evaluation_demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "# Initialize LangSmith client\n",
    "client = Client()\n",
    "\n",
    "# Define dataset: these are your test cases\n",
    "dataset_name = \"QA Example Dataset\"\n",
    "dataset = client.create_dataset(dataset_name)\n",
    "\n",
    "# Define inputs and outputs based on your examples\n",
    "inputs = [\n",
    "    {\"question\": \"Which country is Mount Kilimanjaro located in?\"},\n",
    "    {\"question\": \"What is the chemical symbol for water?\"},\n",
    "    {\"question\": \"Who developed the theory of relativity?\"},\n",
    "    {\"question\": \"Who was the first President of the United States?\"},\n",
    "    {\"question\": \"When did World War II end?\"},\n",
    "    {\"question\": \"Who wrote 'Pride and Prejudice'?\"},\n",
    "    {\"question\": \"What is the title of Shakespeare's play about the Prince of Denmark?\"},\n",
    "    {\"question\": \"Who is considered the founder of Microsoft?\"},\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    {\"answer\": \"Mount Kilimanjaro is located in Tanzania, a country in East Africa known for its diverse wildlife and stunning landscapes.\"},\n",
    "    {\"answer\": \"The chemical symbol for water is H₂O, which represents two hydrogen atoms bonded to one oxygen atom.\"},\n",
    "    {\"answer\": \"The theory of relativity, which revolutionized our understanding of space, time, and gravity, was developed by Albert Einstein in the early 20th century.\"},\n",
    "    {\"answer\": \"The first President of the United States was George Washington, who served from 1789 to 1797 and played a key role in leading the country during its formative years.\"},\n",
    "    {\"answer\": \"World War II ended in 1945, marking the conclusion of one of the most significant and devastating conflicts in human history.\"},\n",
    "    {\"answer\": \"'Pride and Prejudice' was written by Jane Austen, a renowned English novelist known for her insightful commentary on society and human relationships.\"},\n",
    "    {\"answer\": \"The title of Shakespeare's play about the Prince of Denmark is 'Hamlet', a tragedy that explores themes of revenge, madness, and morality.\"},\n",
    "    {\"answer\": \"Microsoft was founded by Bill Gates and Paul Allen in 1975, and it has since become one of the leading technology companies in the world.\"},\n",
    "]\n",
    "\n",
    "# Add examples to the dataset\n",
    "client.create_examples(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    dataset_id=dataset.id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define correctness Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langsmith.evaluation import LangChainStringEvaluator\n",
    "\n",
    "_PROMPT_TEMPLATE = \"\"\"You are an expert professor specialized in grading students' answers to questions.\n",
    "You are grading the following question:\n",
    "{query}\n",
    "Here is the real answer:\n",
    "{answer}\n",
    "You are grading the following predicted answer:\n",
    "{result}\n",
    "Respond with CORRECT or INCORRECT:\n",
    "Grade:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\", \"result\"], template=_PROMPT_TEMPLATE\n",
    ")\n",
    "eval_llm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\": eval_llm, \"prompt\": PROMPT})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "\n",
    "def evaluate_length(run: Run, example: Example) -> dict:\n",
    "    prediction = run.outputs.get(\"output\") or \"\"\n",
    "    required = example.outputs.get(\"answer\") or \"\"\n",
    "    score = int(len(prediction) < 2 * len(required))\n",
    "    return {\"key\":\"length\", \"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Hallucinate Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langsmith.schemas import Run\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    is_grounded: bool = Field(..., description=\"True if the answer is grounded in the facts, False otherwise.\")\n",
    "\n",
    "# LLM with structured outputs for grading hallucinations\n",
    "# For more see: https://python.langchain.com/docs/how_to/structured_output/\n",
    "grader_llm= init_chat_model(model = \"gemma2-9b-it\",model_provider=\"groq\", temperature=0).with_structured_output(\n",
    "    GradeHallucinations,\n",
    "    method=\"json_mode\",\n",
    "    # strict=True,\n",
    ")\n",
    "\n",
    "def no_hallucination(run: Run) -> bool:\n",
    "    \"\"\"Check if the answer is grounded in the documents.\n",
    "\n",
    "    Return True if there is no hallucination, False otherwise.\n",
    "    \"\"\"\n",
    "    # Get documents and answer\n",
    "    qa_pipeline_run = next(\n",
    "        r for r in run.child_runs if r.name == \"qa_pipeline\"\n",
    "    )\n",
    "    retrieve_run = next(\n",
    "        r for r in qa_pipeline_run.child_runs if r.name == \"retrieve\"\n",
    "    )\n",
    "    retrieved_content = \"\\n\\n\".join(\n",
    "        doc[\"page_content\"] for doc in retrieve_run.outputs[\"output\"]\n",
    "    )\n",
    "\n",
    "    # Construct prompt\n",
    "    instructions = (\n",
    "        \"You are a grader assessing whether an LLM generation is grounded in / \"\n",
    "        \"supported by a set of retrieved facts. Give a binary score 1 or 0, \"\n",
    "        \"where 1 means that the answer is grounded in / supported by the set of facts.\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": f\"Set of facts:\\n{retrieved_content}\\n\\nLLM generation: {run.outputs['answer']}\"},\n",
    "    ]\n",
    "\n",
    "    grade = grader_llm.invoke(messages)\n",
    "    return grade.is_grounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target(inputs):\n",
    "  \n",
    "    llm = ChatGroq(model=\"mixtral-8x7b-32768\")\n",
    "    res = llm.invoke(inputs)\n",
    "\n",
    "    return res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langsmith_app(inputs):\n",
    "    # output = my_app(inputs[\"question\"])\n",
    "    llm = ChatGroq(model=\"mixtral-8x7b-32768\")\n",
    "    output = llm.invoke(inputs[\"question\"])\n",
    "    return {\"output\": output.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'cold-volcano-93' at:\n",
      "https://smith.langchain.com/o/f602a986-7b0f-5e2b-aa00-74708f7af432/datasets/cab1b44f-92cb-4aa0-8b0e-55891dd4271f/compare?selectedSessions=92f7f5e6-6dd6-486e-b699-fecd92707f75\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [01:28, 11.11s/it]\n"
     ]
    }
   ],
   "source": [
    "from langsmith import evaluate\n",
    "dataset_name = \"QA Example Dataset\"\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    langsmith_app, # Your AI system\n",
    "    data=dataset_name, # The data to predict and grade over\n",
    "    evaluators=[evaluate_length, qa_evaluator], # The evaluators to score the results\n",
    "    # experiment_prefix=\"openai-3.5\", # A prefix for your experiment names to easily identify them\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.answer</th>\n",
       "      <th>feedback.length</th>\n",
       "      <th>feedback.correctness</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who developed the theory of relativity?</td>\n",
       "      <td>Albert Einstein developed the theory of relati...</td>\n",
       "      <td>None</td>\n",
       "      <td>The theory of relativity, which revolutionized...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>45.409470</td>\n",
       "      <td>84e6e56b-ecf6-45ef-b84d-c511e2d5f01d</td>\n",
       "      <td>922b10d5-048b-4c6e-80c2-b4b642752448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which country is Mount Kilimanjaro located in?</td>\n",
       "      <td>Mount Kilimanjaro is located in Tanzania, a co...</td>\n",
       "      <td>None</td>\n",
       "      <td>Mount Kilimanjaro is located in Tanzania, a co...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45.435957</td>\n",
       "      <td>9b554245-b960-4e2b-ba36-2d62a8c6d9ca</td>\n",
       "      <td>0283f4fa-4702-478a-abe2-c66d9d79347b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the chemical symbol for water?</td>\n",
       "      <td>The chemical formula for water is H2O, not a s...</td>\n",
       "      <td>None</td>\n",
       "      <td>The chemical symbol for water is H₂O, which re...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45.393841</td>\n",
       "      <td>e7a92e73-b60a-4df8-9d12-73201802089b</td>\n",
       "      <td>8f0fba22-fe0f-41f5-9b27-0f4a9520d852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who is considered the founder of Microsoft?</td>\n",
       "      <td>Bill Gates, along with Paul Allen, is consider...</td>\n",
       "      <td>None</td>\n",
       "      <td>Microsoft was founded by Bill Gates and Paul A...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45.514016</td>\n",
       "      <td>2c69f474-66c2-45b6-90d8-9136fe396cf9</td>\n",
       "      <td>f5100ac1-7f72-4412-a8a0-960d3a24e9d2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When did World War II end?</td>\n",
       "      <td>World War II ended officially on September 2, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>World War II ended in 1945, marking the conclu...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45.512490</td>\n",
       "      <td>16e151ca-0386-45be-b4b6-05998ab1d91e</td>\n",
       "      <td>0a9a1490-de52-4bfd-92c1-d57cb0c72e9b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Who wrote 'Pride and Prejudice'?</td>\n",
       "      <td>The author of \"Pride and Prejudice\" is Jane Au...</td>\n",
       "      <td>None</td>\n",
       "      <td>'Pride and Prejudice' was written by Jane Aust...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45.559891</td>\n",
       "      <td>105ac9cc-92f8-47e4-9218-688aa289871b</td>\n",
       "      <td>0218ae01-0d77-45d2-bc89-024af5c72062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Who was the first President of the United States?</td>\n",
       "      <td>George Washington was the first President of t...</td>\n",
       "      <td>None</td>\n",
       "      <td>The first President of the United States was G...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45.827198</td>\n",
       "      <td>25e29272-fe72-4c44-9604-4ae4a05064f7</td>\n",
       "      <td>9f0b1b1d-d597-4ebd-b1cc-c6f0942aa67b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the title of Shakespeare's play about ...</td>\n",
       "      <td>The title of the Shakespeare play you're refer...</td>\n",
       "      <td>None</td>\n",
       "      <td>The title of Shakespeare's play about the Prin...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45.811562</td>\n",
       "      <td>bd818574-e76c-4860-9e7a-4a52854a7569</td>\n",
       "      <td>19315416-0734-419b-9e19-f59dc2e244bd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults cold-volcano-93>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
